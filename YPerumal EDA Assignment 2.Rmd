---
title: "EDA Assignment 2"
author: "Yevashan Perumal"
date: "14/04/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r include=FALSE}
#Import Libaries
library(tidyverse)
library(ggrepel)
library(gridExtra)
```


This report aimns to explore the two datasets "superbru-10k.Rds" and "fixtures.rds". These datasets contain information on predictions made users of the Superbru Super Rugby Fantasy league, and well as information on the fixutres that take place and their related results. 
We then calculate how close the predictions were to the actual results, as well as allocating points to the users as per the Superbru scoring rules.

We then explore the data and attempt to answer various questions surrounding it.

To allocation points, we take the absolute difference between the predicted and actual results. 

As an additional measure of prediction error, we calculate the squared prediction error.



### Instructions to run this script
# Unzip the contents of the folder : ""
Copy the two data files into the same folder
Open the file "STA5092_EDA_Assigment2.Rproj"
Use the file explore to open "Yperumak EDA Assignment 2.Rmd"

```{r}
#Import Dat
# Data in same folder as this file
sup <-read_rds("./superbru-10k.Rds")
fixtures <- read_rds("./fixtures.rds")

# head(sup)
# head(fixtures)
# dim(sup)
# dim(fixtures)
```


```{r}
#Merge predictins and fixtures
df <- sup %>% left_join(fixtures,by='game_id')
dim(df)
head(df)
#Calculate prediction error
df <-df %>% mutate(prediction_error = abs(home_predicted_margin-result))

#correct results indicator
df <- df %>% mutate(correct_result = ifelse(home_predicted_margin>0 & result>0,1,
                                         ifelse(home_predicted_margin<0 & result<0,1,
                                         ifelse(home_predicted_margin==0 & result==0,1,
                                         0))))
```
```{r}
#points allocation function Superbru
points_alloc <- function(result,error){
    if(result==1 & error<=5){
        x<-15
    }else if(result==1 & error>5){
        x<-10
    }else if(result==0 & error<=5){
        x<-5
    }else{
        x<-0
    }
    return(x)
}

df['points_scored'] <- mapply(df$correct_result,df$home_predicted_margin,FUN=points_alloc)
tail(df)
```
```{r}
    # Adding my own measure of prediction accuray, squared errors
#Punishes the degree which you were wrong more
df['squared_error'] <- (df$home_predicted_margin-df$result)^2
head(df)
```
```{r}
# Cumulative points for each user after a game
df <- df %>% group_by(user_id)%>%
    arrange(user_id,game_id)%>%
    mutate(cumulative_score = cumsum(points_scored))%>%
    ungroup()

#Cumulative absolute error
df <- df %>% group_by(user_id)%>%
    arrange(user_id,game_id)%>%
    mutate(cumulative_prediction_error = cumsum(prediction_error))%>%
    ungroup()

df <- df %>% group_by(user_id)%>%
    arrange(user_id,game_id)%>%
    mutate(cumulative_squared_error = cumsum(squared_error))%>%
    ungroup()

```

```{r}
# Task 1
# If games are harder to predict, then prediction errors should be larger.Taking the mean prediction error per game, and then doing a boxplot allows us to find the outliersin terms of mean prediction error(and thus the games hardest to call)


is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
}

b1 <-df %>% group_by(game_id)%>%
    summarise(mean_pred_error=mean(prediction_error))%>%
    mutate(outlier = ifelse(is_outlier(mean_pred_error), 
                            game_id,as.numeric(NA))) %>%
    ggplot(aes(x=1,y=mean_pred_error))+
    geom_boxplot()+
    geom_text_repel(aes(label=outlier),na.rm = TRUE)+
    labs(title = "Histogram of the Mean 
    Prediction Error Per Game",
    subtitle = "Outlier Game ID displayed")+
    ylab("Mean Prediction Error")+
    theme_minimal()

b2 <-df %>% group_by(game_id)%>%
    summarise(mean_sq_pred_error=sqrt(mean(squared_error)))%>%
    mutate(outlier = ifelse(is_outlier(mean_sq_pred_error), 
                            game_id,as.numeric(NA))) %>%
    ggplot(aes(x=1,y=mean_sq_pred_error))+
    geom_boxplot()+
    geom_text_repel(aes(label=outlier),na.rm = TRUE)+
    labs(title = "Histogram of the Mean
    Prediction Error Per Game.",
    subtitle = "Outlier Game ID displayed")+
    ylab("RMSE")+
    theme_minimal()



grid.arrange(b1,b2,ncol=2)
head(df)
```

Displaying Week 1 Leaderboard. All 20 weeks displayed in appendix!!!!!!!!!!
Need to dynamically adjust leaderboard ot deal with ties at rank 20?
If I had to pick a top 20, I pick smallest to display based on user_id (rewards loyalty)

```{r}
#Task 2.1
#Leaderboard
#Do we need to print all twenty we get?
leaderboard<-df %>%group_by(week,user_id)%>%
    summarise(max_score = max(cumulative_score))%>%
    mutate(rank = rank(-max_score,ties.method="first"))%>% #Minus score to rank desc order
    arrange(week,rank)%>%
    filter(rank<=20,week==1)

knitr::kable(leaderboard)
```


```{r}
# Task 2.2
#Code to return rank at any time
game_rank <- function(game_id_input,user_id_input){
    df %>%group_by(game_id,user_id)%>%
    summarise(max_score = max(cumulative_score))%>%
    mutate(rank = rank(-max_score,ties.method="first"))%>% #minus to rank desc order
    arrange(game_id,rank)%>%
    filter(user_id==user_id_input & game_id==game_id_input)
}


knitr::kable(game_rank(game_id_input=1,user_id_input=2))

# Test if the rank stuff is working
```

```{r}
head(df)
```


```{r}
#Task 3

# v1
# df %>% group_by(game_id)%>%
#     mutate(mean_error = mean(prediction_error),med_erro=median(prediction_error))%>%
#     ggplot(aes(x=game_id))+
#     geom_line(aes(y=med_erro-mean_error))
#     # geom_line(aes(y=med_erro))

v2
#get mean and median predictions per game
#also calculate errors related to these prediction values
pred_data<-df %>%select(game_id,home_predicted_margin,result)%>% 
    group_by(game_id)%>%
    arrange(game_id)%>%
    mutate(mean_prediction = round(mean(home_predicted_margin),0) , #Whole number predictions
           median_prediction = median(home_predicted_margin))%>%
    mutate(error_related_to_mean = abs(mean_prediction-result),
             error_related_to_median = abs(median_prediction -result))%>%
    select(game_id,error_related_to_mean,error_related_to_median)%>%
    distinct(game_id,error_related_to_mean,error_related_to_median)

# pred_data %>% ggplot(aes(x=game_id))+
#               geom_line(aes(y=error_related_to_median-error_related_to_mean,color="red"))
#               # geom_line(aes(y=error_related_to_median,colour="red"))

#Plot the prediction errors per game
# pred_data %>% ggplot(aes(x=game_id))+
#               geom_line(aes(y=error_related_to_mean,color="red"))+
#               geom_line(aes(y=error_related_to_median,colour="blue"))

#If the mean guess was always better, then we would have a clear trend of this being positive.
# However, as we can see many fluctuations aruond 0, it doesn't look like this holds
pred_data["difference"] = pred_data$error_related_to_median-pred_data$error_related_to_mean

# Bar Graph
# pred_data %>% ggplot(aes(x=game_id,y=difference))+
#               geom_bar(stat = "identity")

#Line Graphs
pred_data %>% ggplot(aes(x=game_id,y=difference))+
              geom_line()

#Table counting how many times the mean outperformed the median.
pred_data["positive_diff"] <- ifelse(pred_data$difference>0,1,0)
table(pred_data$positive_diff)
# Does not look like wisdom of the crowd held
```


```{r}
# Task 4
# Identify if a prediction was a fan prediction
df <- df%>%mutate(fan_pred = ifelse(user_team_id==home | user_team_id==away,1,0))


#Average error for fan predicted games should be lower?or Higher?
fans <- df%>%group_by(game_id,fan_pred)%>%
    summarise(mean_error = mean(prediction_error))
fans

fans%>% ggplot(aes(x=game_id,y=mean_error,colour=as.factor(fan_pred)))+
    geom_line()

# Variance for fan prediction matches
var(fans[fans$fan_pred==1,'mean_error'])

# Variance for non fan predication matches
var(fans[fans$fan_pred==0,'mean_error'])

#Looks like fans predict their team better

```

```{r}
head(df)
```



```{r}
# Task 5
#120 games
# See who the top 10 are at the halfway point
# first_half <- 
df%>% select(game_id,user_id,cumulative_prediction_error)%>%
      filter(game_id==60)%>%
      arrange(cumulative_prediction_error)%>%
      head(5)


df%>% select(game_id,user_id,cumulative_prediction_error)%>%
      filter(game_id==120)%>%
      arrange(cumulative_prediction_error)%>%
      head(5)


#Maybe compare the ranks at the different points?

```

