---
title: "EDA Assignment 2"
author: "Yevashan Perumal"
date: "14/04/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r include=FALSE}
#Import Libaries
library(tidyverse)
library(ggrepel)
library(gridExtra)
```


This report aimns to explore the two datasets "superbru-10k.Rds" and "fixtures.rds". These datasets contain information on predictions made users of the Superbru Super Rugby Fantasy league, and well as information on the fixutres that take place and their related results. 
We then calculate how close the predictions were to the actual results, as well as allocating points to the users as per the Superbru scoring rules.

We then explore the data and attempt to answer various questions surrounding it.

To allocation points, we take the absolute difference between the predicted and actual results. 

As an additional measure of prediction error, we calculate the squared prediction error.



### Instructions to run this script
# Unzip the contents of the folder : ""
Copy the two data files into the same folder
Open the file "STA5092_EDA_Assigment2.Rproj"
Use the file explore to open "Yperumak EDA Assignment 2.Rmd"

```{r include=FALSE}
#Import Dat
# Data in same folder as this file
sup <-read_rds("./superbru-10k.Rds")
fixtures <- read_rds("./fixtures.rds")

# head(sup)
# head(fixtures)
# dim(sup)
# dim(fixtures)
```


```{r include=FALSE}
#Merge predictions and fixtures
df <- sup %>% left_join(fixtures,by='game_id')
#Calculate prediction error
df <-df %>% mutate(prediction_error = abs(home_predicted_margin-result))

#correct results indicator
df <- df %>% mutate(correct_result = ifelse(home_predicted_margin>0 & result>0,1,
                                         ifelse(home_predicted_margin<0 & result<0,1,
                                         ifelse(home_predicted_margin==0 & result==0,1,
                                         0))))
```


```{r include=FALSE}
#points allocation function Superbru
points_alloc <- function(result,error){
    if(result==1 & error<=5){
        x<-15
    }else if(result==1 & error>5){
        x<-10
    }else if(result==0 & error<=5){
        x<-5
    }else{
        x<-0
    }
    return(x)
}

# Apply the points allocation function
df['points_scored'] <- mapply(df$correct_result,df$home_predicted_margin,FUN=points_alloc)

```


```{r include=FALSE}
# Adding my own measure of prediction accuracy, squared errors
#Punishes the degree which you were wrong more
df['squared_error'] <- (df$home_predicted_margin-df$result)^2
```

```{r include=FALSE}
# Cumulative points for each user after a game
df <- df %>% group_by(user_id)%>%
    arrange(user_id,game_id)%>%
    mutate(cumulative_score = cumsum(points_scored))%>%
    ungroup()

#Cumulative absolute error
df <- df %>% group_by(user_id)%>%
    arrange(user_id,game_id)%>%
    mutate(cumulative_prediction_error = cumsum(prediction_error))%>%
    ungroup()

#Cumulative squared error
df <- df %>% group_by(user_id)%>%
    arrange(user_id,game_id)%>%
    mutate(cumulative_squared_error = cumsum(squared_error))%>%
    ungroup()
```

## Task 1 - Suprising Results
* Some games are harder to predict than others. Find some way of assessing which games were most surprising or difficult to predict for the user base as a whole. Show your results using a plot.

### If games are harder to predict, then prediction errors should be larger.Taking the mean prediction error per game, and then doing a boxplot allows us to find the outliers in terms of mean prediction error(and thus the games hardest to call)


```{r echo=FALSE}
# Task 1
#Fuction to detect outliers in a boxplot
is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
}

# Create plot for mean abs error
b1 <-df %>% group_by(game_id)%>%
    summarise(mean_pred_error=mean(prediction_error))%>%
    mutate(outlier = ifelse(is_outlier(mean_pred_error), 
                            game_id,as.numeric(NA))) %>%
    ggplot(aes(x=1,y=mean_pred_error))+
    geom_boxplot()+
    geom_text_repel(aes(label=outlier),
                    na.rm = TRUE,
                    min.segment.length = 0,
                    force = 3)+
    labs(title = "Histogram of Mean Prediction Error Per Game",
    subtitle = "Outlier Game ID displayed")+
    ylab("Mean Prediction Error")+
    theme_minimal()

# Create plot for root mean squared error
b2 <-df %>% group_by(game_id)%>%
    summarise(mean_sq_pred_error=mean(squared_error))%>%
    mutate(outlier = ifelse(is_outlier(mean_sq_pred_error), 
                            game_id,as.numeric(NA))) %>%
    ggplot(aes(x=1,y=mean_sq_pred_error))+
    geom_boxplot()+
    geom_text_repel(aes(label=outlier),
                    na.rm = TRUE,
                    min.segment.length = 0,
                    force = 3)+
    labs(title = "Histogram of Squared Prediction Error Per Game.",
    subtitle = "Outlier Game ID displayed")+
    ylab("MSE")+
    theme_minimal()


grid.arrange(b1,b2,ncol=2)
```

## Task 2 - Make some Rankings

### 2.1)
* Construct a leaderboard showing the top 20 users after each week (this is something like what Superbru posts online.

The leaderboard for Week 1 is displayed, while the full leaderboard for each week is included in the appendix.

Ties are handled as follows:
If I had to pick a top 20, I pick smallest to display based on user_id. These are users who signed up for Superbru earlier and their loyalty has earned them the priveldge of being displayed. In material terms it makes no difference to the actual points scored.

```{r}
#Task 2.1
#Leaderboard
#Do we need to print all twenty we get?
leaderboard<-df %>%group_by(week,user_id)%>%
    summarise(max_score = max(cumulative_score))%>%
    mutate(rank = rank(-max_score,ties.method="first"))%>% #Minus score to rank desc order
    arrange(week,rank)%>%
    filter(rank<=20,week==1)

knitr::kable(leaderboard)
```

### 2.2)
* Provide code that will return any user’s rank (their position in the rank order from most accurate to least accurate) at any stage in the tournament (i.e. cumulatively, up to any game).

We have provided the result for user_id 2 in game_id 35

```{r echo=FALSE}
# Task 2.2
#Code to return rank at any time
game_rank <- function(game_id_input,user_id_input){
    df %>%group_by(game_id,user_id)%>%
    summarise(max_score = max(cumulative_score))%>%
    mutate(rank = rank(-max_score,ties.method="first"))%>% #minus to rank desc order
    arrange(game_id,rank)%>%
    filter(user_id==user_id_input & game_id==game_id_input)
}


knitr::kable(game_rank(game_id_input=35,user_id_input=2))

# Test if the rank stuff is working
```

## Task 3 - Check for "the wisdom of the crowd"

* Roughly, the “wisdom of the crowd” effect states that if you take a bunch of people and average their prediction, the result will often be pretty good. One way of interpreting this is that the crowd prediction (the average of everyone’s predictions) will be better than the median prediction (this representing an “average” user). Check if the wisdom of the crowd effect holds for these predictions (there are several ways to reasonably do this). Show your results using a plot.

```{r}
#Task 3

# v1
# df %>% group_by(game_id)%>%
#     mutate(mean_error = mean(prediction_error),med_erro=median(prediction_error))%>%
#     ggplot(aes(x=game_id))+
#     geom_line(aes(y=med_erro-mean_error))
#     # geom_line(aes(y=med_erro))

v2
#get mean and median predictions per game
#also calculate errors related to these prediction values
pred_data<-df %>%select(game_id,home_predicted_margin,result)%>% 
    group_by(game_id)%>%
    arrange(game_id)%>%
    mutate(mean_prediction = round(mean(home_predicted_margin),0) , #Whole number predictions
           median_prediction = median(home_predicted_margin))%>%
    mutate(error_related_to_mean = abs(mean_prediction-result),
             error_related_to_median = abs(median_prediction -result))%>%
    select(game_id,error_related_to_mean,error_related_to_median)%>%
    distinct(game_id,error_related_to_mean,error_related_to_median)

# pred_data %>% ggplot(aes(x=game_id))+
#               geom_line(aes(y=error_related_to_median-error_related_to_mean,color="red"))
#               # geom_line(aes(y=error_related_to_median,colour="red"))

#Plot the prediction errors per game
# pred_data %>% ggplot(aes(x=game_id))+
#               geom_line(aes(y=error_related_to_mean,color="red"))+
#               geom_line(aes(y=error_related_to_median,colour="blue"))

#If the mean guess was always better, then we would have a clear trend of this being positive.
# However, as we can see many fluctuations aruond 0, it doesn't look like this holds
pred_data["difference"] = pred_data$error_related_to_median-pred_data$error_related_to_mean

# Bar Graph
# pred_data %>% ggplot(aes(x=game_id,y=difference))+
#               geom_bar(stat = "identity")

#Line Graphs
pred_data %>% ggplot(aes(x=game_id,y=difference))+
              geom_line()

#Table counting how many times the mean outperformed the median.
pred_data["positive_diff"] <- ifelse(pred_data$difference>0,1,0)
table(pred_data$positive_diff)
# Does not look like wisdom of the crowd held
```

## Task 4 - Fan Effects

You might expect fans of a team to be worse at predicting their team’s performance, because they’re biased. Or you might expect them to be better, because they keep up to date with information and know the team well. See if you can assess which one of these explanations is best supported by the data. You don’t need to use any statistical tests, although you can if you want. Show your results using a plot.

```{r}
# Task 4
# Identify if a prediction was a fan prediction
df <- df%>%mutate(fan_pred = ifelse(user_team_id==home | user_team_id==away,1,0))


#Average error for fan predicted games should be lower?or Higher?
fans <- df%>%group_by(game_id,fan_pred)%>%
    summarise(mean_error = mean(prediction_error))
fans

fans%>% ggplot(aes(x=game_id,y=mean_error,colour=as.factor(fan_pred)))+
    geom_line()

# Variance for fan prediction matches
var(fans[fans$fan_pred==1,'mean_error'])

# Variance for non fan predication matches
var(fans[fans$fan_pred==0,'mean_error'])

#Looks like fans predict their team better

```

```{r}
head(df)
```

## Task 5 Find the Experts

* Are there “expert” forecasters? There are many ways of looking at this question. First, think of what expertise means in forecasting. I’d say there are two kinds of expertise: relative expertise and absolute expertise. Relative expertise just means you’re better than most other people. That doesn’t measure absolute quality though. Absolute expertise means that your predictions are in some objective sense good – but this needs some benchmark for defining good.

The goal of this last section is to use the data to assess whether there is consistent good performance over time. To do this, you could divide your data into two periods: a first period that you use to select your experts, and a second period that you use to test whether your experts are actually any good. Again, show your results graphically.


```{r}
# Task 5
#120 games
# See who the top 10 are at the halfway point
# first_half <- 
df%>% select(game_id,user_id,cumulative_prediction_error)%>%
      filter(game_id==60)%>%
      arrange(cumulative_prediction_error)%>%
      head(5)


df%>% select(game_id,user_id,cumulative_prediction_error)%>%
      filter(game_id==120)%>%
      arrange(cumulative_prediction_error)%>%
      head(5)


#Maybe compare the ranks at the different points?

```

