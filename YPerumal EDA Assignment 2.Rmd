---
title: "EDA Assignment 2"
author: "Yevashan Perumal"
date: "18/04/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,message = FALSE, warning = FALSE)
```

```{r include=FALSE}
#Import Libaries
library(tidyverse)
library(ggrepel)
library(gridExtra)
```

## Introduction

This report aims to explore the two data sets: "superbru-10k.Rds" and "fixtures.rds". These data sets contain information on predictions made users of the Superbru Fantasy league, as well as information on the fixtures that take place and their related results. 
Calculations are then performed to determine how close the predictions were to the actual results, as well as allocating points to the users as per the Superbru scoring rules.

The data is then explored, and we attempt to answer various questions surrounding it.

To check how accurate a prediction is,the absolute difference between the predicted and actual results is taken. Additionally, an indicator is created to signal if a prediction was correct or not in terms of match results. Using a combination of these, points are allocated as per the Superbru scoring rules.  

As an additional measure of prediction error, we calculate the squared prediction error.

### Instructions to run this script to reproduce results in this report:
* Unzip the contents of the folder : "STA5092_EDA_Assignment2"
* Copy the two data files(superbru-10k.Rds and fixtures.Rds) into the same folder
* Open the file "STA5092_EDA_Assignment2.Rproj"
* Use the file explorer to open "Yperumal EDA Assignment 2.Rmd"(Double check the working directory is set to current folder where this file was opened)

```{r include=FALSE}
#Import Data
# Data in same folder as this file
sup <-read_rds("./superbru-10k.Rds")
fixtures <- read_rds("./fixtures.rds")

# head(sup)
# head(fixtures)
# dim(sup)
# dim(fixtures)
```


```{r include=FALSE}
#Merge predictions and fixtures
df <- sup %>% left_join(fixtures,by='game_id')
#Calculate prediction error
df <-df %>% mutate(prediction_error = abs(home_predicted_margin-result))

#correct results indicator
df <- df %>% mutate(correct_result = ifelse(home_predicted_margin>0 & result>0,1,
                                         ifelse(home_predicted_margin<0 & result<0,1,
                                         ifelse(home_predicted_margin==0 & result==0,1,
                                         0))))
```


```{r include=FALSE}
#points allocation function Superbru
points_alloc <- function(result,error){
    if(result==1 & error<=5){
        x<-15
    }else if(result==1 & error>5){
        x<-10
    }else if(result==0 & error<=5){
        x<-5
    }else{
        x<-0
    }
    return(x)
}

# Apply the points allocation function
df['points_scored'] <- mapply(df$correct_result,df$home_predicted_margin,FUN=points_alloc)

```


```{r include=FALSE}
# Adding my own measure of prediction accuracy, squared errors
#Punishes the degree which you were wrong more
df['squared_error'] <- (df$home_predicted_margin-df$result)^2
```

```{r include=FALSE}
# Cumulative points for each user after a game
df <- df %>% group_by(user_id)%>%
    arrange(user_id,game_id)%>%
    mutate(cumulative_score = cumsum(points_scored))%>%
    ungroup()

#Cumulative absolute error
df <- df %>% group_by(user_id)%>%
    arrange(user_id,game_id)%>%
    mutate(cumulative_prediction_error = cumsum(prediction_error))%>%
    ungroup()

#Cumulative squared error
df <- df %>% group_by(user_id)%>%
    arrange(user_id,game_id)%>%
    mutate(cumulative_squared_error = cumsum(squared_error))%>%
    ungroup()
```
\newpage
## Task 1 - Suprising Results
* Some games are harder to predict than others. Find some way of assessing which games were most surprising or difficult to predict for the user base as a whole. Show your results using a plot.

If games are harder to predict, then prediction errors should be larger.Taking the mean prediction error per game, and then doing a boxplot allows us to find the outliers in terms of mean prediction error(and thus the games hardest to call). Comparisons are also made with the mean of squared errors, as well as the root mean squared errors.


```{r echo=FALSE}
# Task 1
#Fuction to detect outliers in a boxplot
is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
}

# Create plot for mean abs error
b1 <-df %>% group_by(game_id)%>%
    summarise(mean_pred_error=mean(prediction_error))%>%
    mutate(outlier = ifelse(is_outlier(mean_pred_error), 
                            game_id,as.numeric(NA))) %>% #apply outlier function
    ggplot(aes(x=1,y=mean_pred_error))+
    geom_boxplot()+
    geom_text_repel(aes(label=outlier),
                    na.rm = TRUE,
                    min.segment.length = 0,
                    force = 3)+
    labs(title = ~ atop(paste('Histogram of'),paste("Prediction Error Per Game")),
    subtitle = "Outlier Game ID displayed")+
    ylab("Mean Prediction Error")+
    theme(plot.title = element_text(size = 10,hjust = 0.5),
          plot.subtitle = element_text(size = 8),
          axis.title.x=element_blank())

# Create plot for root mean squared error
b2 <-df %>% group_by(game_id)%>%
    summarise(mean_sq_pred_error=mean(squared_error))%>%
    mutate(outlier = ifelse(is_outlier(mean_sq_pred_error), 
                            game_id,as.numeric(NA))) %>% #apply outlier function
    ggplot(aes(x=1,y=mean_sq_pred_error))+
    geom_boxplot()+
    geom_text_repel(aes(label=outlier),
                    na.rm = TRUE,
                    min.segment.length = 0,
                    force = 3)+
    labs(title = ~ atop(paste('Histogram of Mean Squared'),paste("Prediction Error Per Game")),
    subtitle = "Outlier Game ID displayed")+
    ylab("MSE")+
      theme(plot.title = element_text(size = 10,hjust = 0.5),
          plot.subtitle = element_text(size = 8),
          axis.title.x=element_blank())


b3 <-df %>% group_by(game_id)%>%
    summarise(mean_sq_pred_error=sqrt(mean(squared_error)))%>%
    mutate(outlier = ifelse(is_outlier(mean_sq_pred_error), 
                            game_id,as.numeric(NA))) %>% #apply outlier function
    ggplot(aes(x=1,y=mean_sq_pred_error))+
    geom_boxplot()+
    geom_text_repel(aes(label=outlier),
                    na.rm = TRUE,
                    min.segment.length = 0,
                    force = 3)+
    labs(title = ~ atop(paste('Histogram of'),paste("RMSE Per Game")),
    subtitle = "Outlier Game ID displayed")+
    ylab("RMSE")+
      theme(plot.title = element_text(size = 10,hjust = 0.5),
          plot.subtitle = element_text(size = 8),
          axis.title.x=element_blank())

grid.arrange(b1,b2,b3,ncol=3)
```
It appears that the matches with Game IDs of 29,32 and 2 were the most difficult to predict/surprising results wise. They appear as outliers in all 3 metrics
\newpage

## Task 2 - Make some Rankings

### 2.1)
* Construct a leaderboard showing the top 20 users after each week (this is something like what Superbru posts online.

The leaderboard for Week 1 is displayed, while the full leaderboard for each week is included in the appendix.

Ties are handled as follows:
If a top 20 is to be picked,points scored dictate rankings first. Then we display based on smallest user_id value. These are users who signed up for Superbru earlier, and their loyalty has earned them the privilege of being displayed. In material terms it makes no difference to the actual points scored.

```{r}
#Task 2.1
#Leaderboard
#Do we need to print all twenty we get?
leaderboard<-df %>%group_by(week,user_id)%>%
    summarise(max_score = max(cumulative_score))%>%
    mutate(rank = rank(-max_score,ties.method="first"))%>% #Minus score to rank desc order
    arrange(week,rank)%>%
    filter(rank<=20,week==1)

knitr::kable(leaderboard,
             caption = "Superbru Top 20 Leaderboard",
             align = "lccc",
             booktabs = TRUE,
             col.names = c("Week","User ID","Score","Rank"))
```

### 2.2)
* Provide code that will return any user’s rank (their position in the rank order from most accurate to least accurate) at any stage in the tournament (i.e. cumulatively, up to any game).

We have provided the result for user_id 2 in game_id 35. The code for the function is provided in the appendix.

```{r echo=FALSE}
# Task 2.2
#Code to return rank at any time
game_rank <- function(game_id_input,user_id_input){
    df %>%group_by(game_id,user_id)%>%
    summarise(max_score = max(cumulative_score))%>%
    mutate(rank = rank(-max_score,ties.method="first"))%>% #minus to rank desc order
    arrange(game_id,rank)%>%
    filter(user_id==user_id_input & game_id==game_id_input)
}

knitr::kable(game_rank(game_id_input=35,user_id_input=2),
             caption = "User Rank at a Particular Game in the Season",
             align = "cccc",
             col.names = c("Game","User ID","Score","Rank"))

```
\newpage

## Task 3 - Check for "the wisdom of the crowd"

* Roughly, the “wisdom of the crowd” effect states that if you take a bunch of people and average their prediction, the result will often be pretty good. One way of interpreting this is that the crowd prediction (the average of everyone’s predictions) will be better than the median prediction (this representing an “average” user). Check if the wisdom of the crowd effect holds for these predictions (there are several ways to reasonably do this). Show your results using a plot.

```{r echo=FALSE}
#Task 3

# Get mean and median predictions per game
#also calculate errors related to these prediction values
pred_data<-df %>%select(game_id,home_predicted_margin,result)%>% 
    group_by(game_id)%>%
    arrange(game_id)%>%
    mutate(mean_prediction = round(mean(home_predicted_margin),0) , #Whole number predictions
           median_prediction = median(home_predicted_margin))%>%
    mutate(error_related_to_mean = abs(mean_prediction-result),
             error_related_to_median = abs(median_prediction -result))%>%
    select(game_id,error_related_to_mean,error_related_to_median)%>%
    distinct(game_id,error_related_to_mean,error_related_to_median) #remove duplicates caused by user level data

# Calculate difference between median and mean prediction errors
pred_data["difference"] = pred_data$error_related_to_median-pred_data$error_related_to_mean

# Plot the Errors
# pred_data %>% ggplot(aes(x=game_id))+
#               geom_line(aes(y=error_related_to_median-error_related_to_mean,color="red"))
#               # geom_line(aes(y=error_related_to_median,colour="red"))

#Plot the prediction errors per game
# pred_data %>% ggplot(aes(x=game_id))+
#               geom_line(aes(y=error_related_to_mean,color="red"))+
#               geom_line(aes(y=error_related_to_median,colour="blue"))

# Plot the Difference
pred_data %>% ggplot(aes(x=game_id,y=difference))+
              geom_line(color="blue")+
              labs(title = "The Difference in Median and Mean Prediction Errors Over the Season")+xlab("Game ID")+ylab("Difference in Error Metrics")
```
The graph above shows the difference between the median prediction error and the mean prediction error per game. If the mean prediction(i.e. the wisdom of the crowds) is better than the average(given by the median), then the prediction error for the median should always be larger, and thus the difference between the two values per game should have more values that are positive than not. However, we see many fluctuations around 0 as the games progress through the season.

To get a clearer picture of this, we determine how many instances the difference in these values is positive i.e. a "Wise" crowd. 

```{r}
#Table counting how many times the mean outperformed the median.
pred_data["positive_diff"] <- ifelse(pred_data$difference>0,"Wise Crowd","Unwise Crowd")

pred_data%>%ggplot(aes(x=positive_diff,))+
      geom_bar()+
      labs(title = "Frequency for Wise vs Unwise Crowd Predictions")+
      xlab("Crowd Designation")+
      ylab("Frequency")+
      geom_text(stat='count', aes(label=..count..), vjust=-0.3)
# knitr:: kable(table(pred_data$positive_diff))
# Does not look like wisdom of the crowd held
```
If we look at the actual frequency of where the difference was positive and apply the logic above, it appears the wisdom of the crowd does not hold as there are many more instance where the  crowd's median prediction outperformed the crowd's mean  prediction.
\newpage

## Task 4 - Fan Effects

You might expect fans of a team to be worse at predicting their team’s performance, because they’re biased. Or you might expect them to be better, because they keep up to date with information and know the team well. See if you can assess which one of these explanations is best supported by the data. You don’t need to use any statistical tests, although you can if you want. Show your results using a plot.

```{r echo=FALSE}
# Task 4
# Identify if a prediction was a fan prediction
df <- df%>%mutate(fan_pred = ifelse(user_team_id==home | user_team_id==away,"Fan","Non-Fan"))

#Average error for fan predicted games should be lower?or Higher?
fans <- df%>%group_by(game_id,fan_pred)%>%
    summarise(mean_error = mean(prediction_error))

fans%>% ggplot(aes(x=game_id,y=mean_error,colour=fan_pred))+
    geom_line()+
    labs(title = ~ atop(paste('Plot Displaying Mean Absolute Error per Game'),paste("of Fan vs Non Fan Predictions")))+
    xlab("Game ID")+
    ylab("Mean Absolute Error")+
    theme(legend.title = element_blank(),plot.title = element_text(hjust = 0.5))
```
The figure above plots the average error for fan predictions vs non fan predictions per game. It appears that fan prediction have slightly less variability in the errors and therefore may be slightly better than non fan predictions. However, it is difficult to draw substantial conclusions from this view.We can confirm this by looking at the actual variance for each subset of predictions:

```{r}
# Variance for fan prediction matches
fan_var = round(var(fans[fans$fan_pred=="Fan",'mean_error']),2)

# Variance for non fan predication matches
non_fan_var=round(var(fans[fans$fan_pred=="Non-Fan",'mean_error']),2)

#Looks like fans predict their team better
```

### Error Variance for games predicted by fans: `r fan_var`
### Error Variance for games predicted by fans: `r non_fan_var`

Games predicted by fans have a lower variance in their errors, and therefore appear to be more dialed in to produce accurate results,indicating fans are able to make better predictions for the teams they support.



```{r eval=FALSE, include=FALSE}
head(df)
```
\newpage
## Task 5 Find the Experts

* Are there “expert” forecasters? There are many ways of looking at this question. First, think of what expertise means in forecasting. I’d say there are two kinds of expertise: relative expertise and absolute expertise. Relative expertise just means you’re better than most other people. That doesn’t measure absolute quality though. Absolute expertise means that your predictions are in some objective sense good – but this needs some benchmark for defining good.

The goal of this last section is to use the data to assess whether there is consistent good performance over time. To do this, you could divide your data into two periods: a first period that you use to select your experts, and a second period that you use to test whether your experts are actually any good. Again, show your results graphically.


To find our "experts" we have followed this approach:

1. Split the data into two periods, the first 60 games and the second 60 games
2. In the first 60 games we plot the total points and total error per user
3. To determine our experts, we calculate the "error per point" ratio for each user 
4. The top 100 user IDs are highlighted. This means for each point earned, these users were as close as could be with their prediction to the actual result.
5. A similar process followed in step 3 for the second 60 games of the season
6. To find the "experts" we overlay the top 100 users from the second half of the season with those from the first to check if any user appears in the top 100 in both periods.

The plots are displayed below:

```{r}
# Task 5
#120 games
first_half <-df%>% select(game_id,user_id,prediction_error,points_scored)%>%
      filter(game_id<=60) %>%
      group_by(user_id)%>%
      summarise(total_points=sum(points_scored),total_abs_error=sum(prediction_error))%>%
      mutate(error_per_point = total_abs_error/total_points)%>% #eror per points ratio
      mutate(top_100 = ifelse(rank(error_per_point)<=100,"Top 100","The Rest")) #mark top 100

#Get top 100 user ids to compare in second half of season
first_half_top_100_users <- first_half%>%filter(top_100=="Top 100")%>%select(user_id)

# Plot first half of season
s1 <- first_half%>%ggplot(aes(y=total_points,x=total_abs_error))+
              geom_point(aes(color=top_100))+
              labs(title =  ~ atop(paste('Scatterplot of Total Points vs Total Error per User'),paste("First 60 Games")),
                   subtitle = "Users with 100 lowest error per point ratio highlighted")+
              xlab("Total Error")+
              ylab("Total Points")+
              scale_color_discrete(name = "Error Per Point Rank")+
              theme(plot.title = element_text(hjust = 0.5))

#Second half of the season
second_half <-df%>% select(game_id,user_id,prediction_error,points_scored)%>%
      filter(game_id>60) %>%
      group_by(user_id)%>%
      summarise(total_points=sum(points_scored),total_abs_error=sum(prediction_error))%>%
      mutate(error_per_point = total_abs_error/total_points)%>%
      mutate(top_100 = ifelse(rank(error_per_point)<=100,"Top 100","The Rest"))%>%
      mutate(first_half_top_100 = ifelse(user_id %in% first_half_top_100_users$user_id,1,0))%>% #indictor for top 100 users from first half of season
      mutate(combine_t100 = ifelse(top_100=="Top 100"& first_half_top_100==1,"Expert",top_100)) #Combined indicator of top 100 to check if any users repeat from first half

s2 <- second_half%>%ggplot(aes(y=total_points,x=total_abs_error))+
              geom_point(aes(color=combine_t100))+
              labs(title =  ~ atop(paste('Scatterplot of Total Points vs Total Error per User'),paste("Second 60 Games")),
                   subtitle = "Users with 100 lowest error per point ratio highlighted")+
              xlab("Total Error")+
              ylab("Total Points")+
              scale_color_discrete(name = "Error Per Point Rank")+
              theme(plot.title = element_text(hjust = 0.5))

s1
```

```{r}
s2
```

It appears that experts to exist; However, they appear to be a very rare commodity as shown in the table below.
```{r}
knitr::kable(table(second_half$combine_t100),
             caption = "Breakdown of Top 100, Second 60 Games",
             col.names = c("Category","No. of Users"))
```

\newpage

## Appendix

```{r Full Leaderboard, echo=FALSE}
leaderboard<-df %>%group_by(week,user_id)%>%
    summarise(max_score = max(cumulative_score))%>%
    mutate(rank = rank(-max_score,ties.method="first"))%>% #Minus score to rank desc order
    arrange(week,rank)%>%
    filter(rank<=20)

knitr::kable(leaderboard,
             caption = "Superbru Top 20 Leaderboard",
             align = "lccc",
             booktabs = TRUE,
             col.names = c("Week","User ID","Score","Rank"))
```

```{r echo=TRUE}
#Code to return rank at any time
game_rank <- function(game_id_input,user_id_input){
    df %>%group_by(game_id,user_id)%>%
    summarise(max_score = max(cumulative_score))%>%
    mutate(rank = rank(-max_score,ties.method="first"))%>% #minus to rank desc order
    arrange(game_id,rank)%>%
    filter(user_id==user_id_input & game_id==game_id_input)
}

knitr::kable(game_rank(game_id_input=35,user_id_input=2),
             caption = "User Rank at a Particular Game in the Season",
             align = "cccc",
             col.names = c("Game","User ID","Score","Rank"))
```

